\subsection{Адаптивный рандомизированный\\ жадный алгоритм (ARG)}
\label{sub:arg}


\subsubsection*{Применимость алгоритма SPSA. Функция качества}

Применимость \emph{SPSA} обоснована теоретически для выпуклой усредненной функции качества.

В зависимости от графа усредненная модулярность результатов работы $RG_k$ либо принимает максимум при небольшом $k$, как на рисунке \ref{fig:spsa_validity_karate}, либо постепенно увеличивается при росте $k$, как на рисунке \ref{fig:spsa_validity_netscience}. Значение $k$, при которым $RG_k$ будет на графе принимать наилучшее значение, далее в работе называется \emph{оптимальным} $k$ или $k_{opt}$.

\begin{figure}[h!]
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{prodanov/prodanov_validity_karate.eps}
		\caption{Зависимость модулярности от $k$ на графе \emph{karate}}
		\label{fig:spsa_validity_karate}
	\end{subfigure}
	%
	\begin{subfigure}{.1\textwidth}
	\end{subfigure}
	%
	\begin{subfigure}{.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{prodanov/prodanov_validity_netscience.eps}
		\caption{Зависимость модулярности от $k$ на графе \emph{netscience}}
		\label{fig:spsa_validity_netscience}
	\end{subfigure}
\caption{Зависимость модулярности от параметра $k$ на двух графах}
\label{fig:spsa_validity}
\end{figure}

Время работы $RG_k$ линейно растет при росте $k$, поэтому предлагается следующая функция качества:
\begin{equation} \label{eq:quality_function}
	f(Q, k) = -\ln Q + \beta \ln k.
\end{equation}

Коэффициент $\beta \ge 0$ можно рассматривать в виде $\beta = \frac{\ln \gamma}{\ln 2},$ где $\gamma$ указывает, во сколько раз можно позволить увеличиться модулярности, чтобы покрыть увеличение времени (то есть $k$) вдвое.

\begin{figure}[h!]
	\begin{subfigure}{.465\textwidth}
		\centering
		\includegraphics[width=\linewidth]{prodanov/prodanov_quality_karate.eps}
		\caption{Функция качества на графе \emph{karate}}
		\label{fig:spsa_quality_karate}
	\end{subfigure}
	%
	\begin{subfigure}{.1\textwidth}
	\end{subfigure}
	%
	\begin{subfigure}{.435\textwidth}
		\centering
		\includegraphics[width=\linewidth]{prodanov/prodanov_quality_netscience.eps}
		\caption{Функция качества на графе \emph{netscience}}
		\label{fig:spsa_quality_netscience}
	\end{subfigure}
\caption{Функция качества при разных значения значениях $\beta$ на двух графах}
\label{fig:spsa_quality}
\end{figure}

Как видно из рисунка \ref{fig:spsa_quality_netscience}, иногда для того, чтобы функция качества имела экстремум --- надо задавать довольно большое значение коэффициента $\beta$. Однако это логично, если время работы алгоритма удовлетворительно --- выгодно все время увеличивать значение параметра $k$. Однако, как будет показано дальше, есть небольшие значения $\beta$ дают хорошие результаты на всех тестовых графах.


\subsubsection*{Адаптивный рандомизированный жадный алгоритм}

Для использования алгоритма \emph{SPSA} в рандомизированном жадном алгоритме предлагается разбить действие алгоритма на шаги длиной в $\sigma$ итераций. В течении каждого шага используется одно значение $k$, а после каждых двух шагов подбираются следующие два значения $k$, которые будут ближе к тому из предыдущих значений $k$, которое дало меньшую функцию качества.

В функции качества используется медиана прироста модулярности за $\sigma$ шагов. Это связано с тем, что алгоритм рандомизированный, время от времени будут появляться очень хорошие соединения сообществ, которые будут портить функцию качества, такой большой прирост может появиться даже при очень плохом $k$. В таком случае схема алгоритма будет выглядеть следующим образом:

\begin{enumerate}
	\item Выбирается начальная оценка $\hat{k}_0 \in \mathbb{N}$, размер возбуждения $d \in \mathbb{N}$, чувствительность к перепадам функций качества $\alpha \in \mathbb{R},\; \alpha > 0$, значимость времени $\beta \in \mathbb{R},\; \beta \ge 0$, и количество $\sigma \in \mathbb{N}$ итераций в одном шаге. Устанавливается счетчик $n \leftarrow 0$
	\item Увеличивается счетчик $n \leftarrow n + 1$
	\item Определяются следующие два параметра\\ $k_{n}^{-} \leftarrow \max\{\hat{k}_{n - 1} - d, 1\}$ и $k_{n}^{+} \leftarrow \hat{k}_{n - 1} + d$
	\item $\sigma$ итераций рассматриваются $k_n^{-}$ случайных сообществ и все их соседей, соединяется пара соседей, дающая лучший прирост модулярности. Медиана прироста модулярности за $\sigma$ итераций обозначается $\mu_n^{-}$
	\item $\sigma$ итераций рассматриваются $k_n^{+}$ случайных сообществ и все их соседей, соединяется пара соседей, дающая лучший прирост модулярности. Медиана прироста модулярности за $\sigma$ итераций обозначается $\mu_n^{+}$
	\item Вычисляется функции качества\\ $y_n^{-} \leftarrow -\ln \mu_n^{-} + \beta \ln k_n^{-}$ и $y_n^{+} \leftarrow -\ln \mu_n^{+} + \beta \ln k_n^{+}$
	\item Вычисляется следующая оценка
	\begin{equation} \label{eq:arg-centre}
		\hat{k}_n \leftarrow \max\left\{1, \left[\hat{k}_{n - 1} - \alpha \frac{y_n^{+} - y_n^{-}}{k_n^{+} - k_n^{-}}\right]\right\}
	\end{equation}
	\item Далее происходит переход на второй пункт
\end{enumerate}

Алгоритм заканчивает работу в тот момент, когда для рассмотрения осталось ровно одно сообщество. Далее в статье этот алгоритм называется \emph{адаптивным рандомизированным жадным алгоритмом} или \emph{ARG} (Adaptive Randomized Greedy).


\subsubsection*{Исследование работоспособности при разных параметрах}

В отличии от $RG$, имевшего один параметр --- $ARG$ имеет пять независимых параметров. Однако, как показали эмпирические исследования на графах \emph{cond-mat-2003}, \emph{caidaRouterLevel} и \emph{cnr-2000}, чувствительность к перепадам функций качества $\alpha$ и количество $\sigma$ итераций в шаге слабо влияют на результаты вычислений. Размер возмущения $d$ и начальная оценка $\hat{k}_0$ оказывают большее влияние на результаты, однако для них существуют значения, дающие хорошие результаты на каждом тестовом графе. Например, хорошие результаты будет давать набор параметров $\alpha = 10,\;\sigma = 1000,\;d = 5,\;\hat{k}_0 = 8$.

При увеличении значения коэффициента значимости времени $\beta$ время работы алгоритма уменьшается за счет уменьшения работы, однако часто при небольших значениях $\beta$ алгоритм дает более хорошие разбиения, чем при нулевом $\beta$.

\begin{figure}[h!]
	\begin{subfigure}{.46\textwidth}
		\centering
		\includegraphics[width=\linewidth]{prodanov/prodanov_arg_beta_q.eps}
		\caption{Зависимость модулярности от $\beta$ в работе $ARG$}
		\label{fig:arg_beta_q}
	\end{subfigure}
	%
	\begin{subfigure}{.1\textwidth}
	\end{subfigure}
	%
	\begin{subfigure}{.44\textwidth}
		\centering
		\includegraphics[width=\linewidth]{prodanov/prodanov_arg_beta_t.eps}
		\caption{Зависимость времени работы $t$ алгоритма $ARG$ от $\beta$}
		\label{fig:arg_beta_t}
	\end{subfigure}
\caption{Зависимости модулярности и времени работы $ARG$ от коэфициента значимости времени $\beta$ на графе \emph{cond-mat-2003}}
\label{fig:arg_beta}
\end{figure}


\subsubsection*{Сравнение RG и ARG}

Для сравнения алгоритмов $RG$ и $ARG$ сопостовлялась медианная модулярность разбиений тестовых графов. В таблицах \ref{tab:comparison_rg_arg_q} и \ref{tab:comparison_rg_arg_t} указаны медианные модулярности разбиений, полученных с помощью $RG$ с разными значениям параметра $k$:
\begin{itemize}
	\item $k = 1$ --- минимально возможное значение
	\item $k = 3$ --- значение, чаще остальных дающее лучшие значения
	\item $k = 10$ --- значение, дающее стабильные результаты
	\item $k = 50$ --- значение, показывающее поведение $RG$ при больших значениях параметра $k$
\end{itemize}

Алгоритм $ARG$ запускался со следующими параметрами:\\$\alpha = 10,\;\sigma = 1000,\;d= 5,\;\hat{k}_0 = 8,\;\beta = 0.05$.

\begin{table}[h!]
	\caption{Медианные модулярности разбиений, полученных в результате работы $RG_k$ с разными $k$ и $ARG$ на тестовых графах}
 	\label{tab:comparison_rg_arg_q}
 	\begin{tabularx}{\textwidth}{Xrrrrrrrrr}\hline
 						& $RG_1$	& $RG_3$	& $RG_{10}$	& $RG_{50}$	& $ARG$		\\\hline
 	as-22july06			& 0.65281	& 0.64658	& 0.64024	& 0.63479	& 0.64264	\\
 	cond-mat-2003		& 0.00012	& 0.19727	& 0.70738	& 0.69403	& 0.71193	\\
 	auto40			 	& 0.78944	& 0.79988	& 0.80417	& 0.80273	& 0.80174	\\
 	caidaRouterLevel 	& 0.01938	& 0.81101	& 0.79883	& 0.79300	& 0.80216	\\
 	cnr-2000			& 0.90237	& 0.91192	& 0.91144	& 0.90997	& 0.91039	\\
 	eu-2005				& 0.92765	& 0.92559	& 0.91780	& 0.90416	& 0.91048	\\
 	in-2004				& 0.00026	& 0.97836	& 0.97185	& 0.97596	& 0.97616	\\\hline
 	\end{tabularx}
\end{table}

\begin{table}[h!]
	\caption{Время работы (в миллисекундах) $RG_k$ с разными $k$ и $ARG$ на тестовых графах}
 	\label{tab:comparison_rg_arg_t}
 	\begin{tabularx}{\textwidth}{Xrrrrrrrrr}\hline
 						& $RG_1$	& $RG_3$	& $RG_{10}$	& $RG_{50}$	& $ARG$		\\\hline
 	as-22july06			& 177		& 189		& 231		& 464		& 238		\\
 	cond-mat-2003		& 58		& 184		& 463		& 931		& 474		\\
 	auto40			 	& 4,652		& 4,591		& 6,017		& 12,558	& 6,479		\\
 	caidaRouterLevel 	& 852		& 9,114		& 10,244	& 15,217	& 11,514	\\
 	cnr-2000			& 26,083	& 26,056	& 27,137	& 33,592	& 29,054	\\
 	eu-2005				& 202,188	& 200,686	& 207,689	& 246,170	& 225,748	\\
 	in-2004				& 9,208		& 487,953	& 553,196	& 607,408	& 617,345	\\\hline
 	\end{tabularx}
\end{table}

В большинстве случаев один или несколько параметров $k$ дают $RG_k$ лучшие результаты, чем результат $ARG$, однако адаптивный алгоритм дает более стабильные результаты. $ARG$ можно сравнить по стабильности с $RG_{10}$, однако в среднем $ARG$ дает б\'{о}льшую модулярность.


\subsubsection*{Применение ARG в качестве начального алгоритма CGGC}

Время работы схемы основных групп графа ($CGGC$) складывается из времени работы $s$ начальных алгоритмов и времени работы финального алгоритма. Поэтому имеет смысл в качестве начальных алгоритмов брать алгоритмы, работающие быстро. Однако в случае, если начальные алгоритмы очень плохие, то есть дают разбиения не лучше случайного --- модулярность разбиений, полученных \emph{CGGC} будет приблизительно равна модулярности разбиений, полученных финальным алгоритмом.

Если граф достаточно большой (количество узлов как минимум в несколько раз больше $2\sigma$) --- на нем в качестве начального алгоритма имеет смысл использовать $ARG$, чтобы получать стабильные результаты.

\begin{table}[h!]
	\caption{Модулярность разбиений, полученных в результате работы \emph{CGGC} с начальным алгоритмом $A_{init}$ и финальным алгоритмом $A_{final}$ на трех графах}
	\label{tab:arg_in_cggc_q}
	{\footnotesize
	\begin{tabularx}{\textwidth}{XR{0.9cm}R{0.9cm}R{0.9cm}R{0.9cm}R{0.9cm}R{0.9cm}} \hline
	$A_{init}$	& \multicolumn{2}{c}{$RG_3$} & \multicolumn{2}{c}{$RG_{10}$} & \multicolumn{2}{c}{$ARG$} \\
	$A_{final}$ & \multicolumn{1}{c}{$RG_3$} & \multicolumn{1}{c}{$RG_{10}$} & \multicolumn{1}{c}{$RG_3$} & \multicolumn{1}{c}{$RG_{10}$} & \multicolumn{1}{c}{$RG_3$} & \multicolumn{1}{c}{$RG_{10}$} \\\hline
	cond-mat-2003 		& 0.16840	& 0.71155	& 0.44934	& 0.74794	& 0.42708	& 0.74872	\\
	auto40				& 0.80628	& 0.80645	& 0.80633	& 0.80645	& 0.80628	& 0.80647	\\
	caidaRouterLevel	& 0.84078	& 0.85372	& 0.84031	& 0.84448	& 0.83671	& 0.85279	\\\hline
	\end{tabularx}
	}
\end{table}

\begin{table}[h!]
	\caption{Время работы \emph{CGGC} с начальным алгоритмом $A_{init}$ и финальным алгоритмом $A_{final}$ на трех графах}
	\label{tab:arg_in_cggc_t}
	{\footnotesize
	\begin{tabularx}{\textwidth}{XR{0.9cm}R{0.9cm}R{0.9cm}R{0.9cm}R{0.9cm}R{0.9cm}} \hline
	$A_{init}$	& \multicolumn{2}{c}{$RG_3$} & \multicolumn{2}{c}{$RG_{10}$} & \multicolumn{2}{c}{$ARG$} \\
	$A_{final}$ & \multicolumn{1}{c}{$RG_3$} & \multicolumn{1}{c}{$RG_{10}$} & \multicolumn{1}{c}{$RG_3$} & \multicolumn{1}{c}{$RG_{10}$} & \multicolumn{1}{c}{$RG_3$} & \multicolumn{1}{c}{$RG_{10}$} \\\hline
	cond-mat-2003 		& 2.0	& 2.3	& 4.7	& 4.8	& 4.8	& 4.9	\\
	auto40				& 47.0	& 46.6	& 57.7	& 57.2	& 65.7	& 65.8	\\
	caidaRouterLevel	& 94.0	& 93.7	& 104.2	& 104.3	& 115.1	& 118.5	\\\hline
	\end{tabularx}
	}
\end{table}

На таблицах \ref{tab:arg_in_cggc_q} и \ref{tab:arg_in_cggc_t} сравниваются результаты работы \emph{CGGC} с разными начальными и финальными алгоритмами. В качестве начального алгоритма используеся $RG_k$ с параметрами $k = 3$ и $k = 10$, и $ARG$ с параметрами $\alpha = 10,\;\sigma = 1000,\;d= 5,\;\hat{k}_0 = 8,\;\beta = 0.02$. В качестве финального алгоритма используются только $RG_3$ и $RG_{10}$, так как на рассматриваемых графах промежуточное разбиение состоит из недостаточно большого количества сообществ, чтобы использовать \emph{ARG}.

Граф \emph{cond-mat-2003} плохо разбивается случайными жадными алгоритмами с маленьким $k$, и если обратить внимание на таблицу \ref{tab:arg_in_cggc_q} --- в случаях, когда $RG_3$ используется на этом графе в качестве начального алгоритма --- модулярность \emph{CGGC} сравнима с модулярностью финального алгоритма на этом графе (модулярности $RG_3$ и $RG_{10}$ на разных графах можно увидеть в таблице \ref{tab:comparison_rg_arg_q}). В случаях, когда $RG_3$ используется в качестве финального алгоритма --- модулярность получается сравнительно плохой.

Заранее неизвестно, при каких $k$ алгоритм будет хорошо работать, а при каких плохо, кроме того неизвестно по результату, хорошее ли это разбиение для конкретного графа или нет. \emph{CGGC} работает достаточно долго, поэтому запускать \emph{CGGC} с разными начальными алгоритмами несколько раз для определения хороших параметров не выгодно. Таким образом, выгодно использовать $ARG$ и в качестве начального алгоритма, и в качестве финального, если промежуточное разбиение подходит по размерам.
